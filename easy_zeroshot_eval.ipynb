{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e48e4f74-3341-420b-b94e-4db1bd33c789",
   "metadata": {},
   "source": [
    "# Performing Zero-shot Evaluation of a Statement Tuned Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79ac5b4-e566-432b-91de-ab2f9d2ca76d",
   "metadata": {},
   "source": [
    "Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "238e1aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, get_dataset_config_names, Dataset\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizerFast\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from sklearn.utils import resample\n",
    "import evaluate\n",
    "from sklearn.utils import resample\n",
    "from copy import copy\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6738e223-04f7-4acb-ac44-931c910c25f1",
   "metadata": {},
   "source": [
    "Setting the random seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7041a1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "NUM_PROC=5\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6d50dd-25e1-47f5-b3df-9d30c7db465d",
   "metadata": {},
   "source": [
    "Helper function to assist with creating prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f357a37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_template(templates, values):\n",
    "    temp = random.sample(templates,1)[0]\n",
    "    for i in range(len(values)):\n",
    "        temp = temp.replace(\"${\"+str(i+1)+\"}\", values[i])\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80825c25-df47-4149-903b-4bedc0a9134f",
   "metadata": {},
   "source": [
    "Creating two statements to choose from for each data example. One has choice1 the other has choice2. The idea is that the statement tuned model is expected to give the correct choice the higher score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "98e55996",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"xcopa\"\n",
    "templates = [[\"The cause of \\\"${1}\\\" is that \\\"${2}\\\"\", \"\\\"${1}\\\" because \\\"${2}\\\"\", \"\\\"${1}\\\" due to \\\"${2}\\\"\"], [\"The effect of \\\"${1}\\\" is that \\\"${2}\\\"\", \"\\\"${1}\\\" therefore \\\"${2}\\\"\", \"\\\"${1}\\\", so \\\"${2}\\\"\"]]\n",
    "split = ['test']\n",
    "label_column = 'label'\n",
    "question = 'premise'\n",
    "choices = ['choice1', 'choice2']\n",
    "langs = get_dataset_config_names(dataset)[:9]\n",
    "data = {}\n",
    "for lang in langs:\n",
    "    data[lang] = load_dataset(dataset, lang, split=split)\n",
    "col_names = copy(data[langs[0]][0].column_names)\n",
    "col_names.remove(label_column)\n",
    "def create_statements_labels_copa(example):\n",
    "    template = templates[0] if example['question'] == 'cause' else templates[1]\n",
    "    temp = random.choice(template)\n",
    "    example['statement1'] = fill_template([temp], [example[question], example[choices[0]]])\n",
    "    example['statement2'] = fill_template([temp], [example[question], example[choices[1]]])\n",
    "    return example\n",
    "\n",
    "xcopa_statements = {}\n",
    "for lang in langs:\n",
    "    xcopa_statements[lang] = [split.map(create_statements_labels_copa, remove_columns=col_names, num_proc=NUM_PROC) for split in data[lang]][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2ef6e4-05f8-42a2-8dbe-a9a657cc2cd2",
   "metadata": {},
   "source": [
    "You can view what happens when we convert the problem into statements here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1c41e210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': [0, 0, 1, 0, 0],\n",
       " 'statement1': ['The cause of \"Ese oli mullikilesse mässitud.\" is that \"See oli õrn.\"',\n",
       "  'The effect of \"Ma tühjendasin oma taskud.\" is that \"Ma leidsin pileti tüki.\"',\n",
       "  '\"Termiidid tungisid majja sisse.\" therefore \"Termiidid kadusid majast.\"',\n",
       "  '\"Reisidjad jõudsid piirini.\" therefore \"Piirikontroll kontrollis nende passe.\"',\n",
       "  '\"Kontor oli kinni.\" because \"Oli puhkus.\"'],\n",
       " 'statement2': ['The cause of \"Ese oli mullikilesse mässitud.\" is that \"See oli väike.\"',\n",
       "  'The effect of \"Ma tühjendasin oma taskud.\" is that \"Ma leidsin relva.\"',\n",
       "  '\"Termiidid tungisid majja sisse.\" therefore \"Termiidid sõid läbi majas oleva puidu.\"',\n",
       "  '\"Reisidjad jõudsid piirini.\" therefore \"Piirikontroll süüdistas neid smuugeldamises.\"',\n",
       "  '\"Kontor oli kinni.\" because \"Oli suvi.\"']}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xcopa_statements['et'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5c145877",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b898bd99-cc16-418f-9704-1b5916206069",
   "metadata": {},
   "source": [
    "In this case we use the same tokenizer as roberta-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9ba1f68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b6636b-bfdf-48a2-8c75-ddddeb7915a7",
   "metadata": {},
   "source": [
    "English only statement tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8642922c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ashabrawy/ST-roberta-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5b55e7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(model_name, cache_dir=\"/scratch/afz225/.cache\").eval().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505b6fd2-152c-4018-af0c-bffbc37effe4",
   "metadata": {},
   "source": [
    "We create dataloaders for each language to be able to load the data in batches for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c1708abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "xcopa_dataloaders = {}\n",
    "for lang in langs:\n",
    "    xcopa_dataloaders[lang] = DataLoader(xcopa_statements[lang], batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b67d8b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bec08ea-3ee0-4e03-b424-adaf02fc48b4",
   "metadata": {},
   "source": [
    "The following cell runs both statements/choices through the statement tuned model. We get the logits for each statement, combine them then use argmax to find the choice that gives the higher probability. This is our prediction. We use these predictions with the correct labels to calculate the prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b1555df1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lang_accuracies = {}\n",
    "for lang in langs:\n",
    "    predictions = []\n",
    "    actual_labels = []\n",
    "    for batch in xcopa_dataloaders[lang]:\n",
    "        tok1 = tokenizer(batch['statement1'], return_tensors='pt', padding=True).to(device)\n",
    "        tok2 = tokenizer(batch['statement2'], return_tensors='pt', padding=True).to(device)\n",
    "        labels = batch['label']\n",
    "        prob1 = F.softmax(model(input_ids=tok1['input_ids'], attention_mask=tok1['attention_mask']).logits, dim=-1)[:,1]\n",
    "        prob2 = F.softmax(model(input_ids=tok2['input_ids'], attention_mask=tok2['attention_mask']).logits, dim=-1)[:,1]\n",
    "        preds = torch.argmax(torch.stack([prob1, prob2],dim=-1),dim=-1)\n",
    "        predictions.extend(preds.cpu().tolist())\n",
    "        actual_labels.extend(labels.cpu().tolist())\n",
    "    lang_accuracies[lang] = clf_metrics.compute(predictions=predictions, references=actual_labels)['accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8729f6-04c1-4696-96f5-0264872ac78e",
   "metadata": {},
   "source": [
    "Low scores for lang_accuracies expected because our model is trained only on English!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9ee22b0b-cb7f-411a-92b0-7a567badbe14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'et': 0.484,\n",
       " 'ht': 0.526,\n",
       " 'id': 0.498,\n",
       " 'it': 0.524,\n",
       " 'qu': 0.53,\n",
       " 'sw': 0.468,\n",
       " 'ta': 0.47,\n",
       " 'th': 0.478,\n",
       " 'tr': 0.51}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_accuracies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw1",
   "language": "python",
   "name": "hw1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
